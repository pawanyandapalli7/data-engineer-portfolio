# Data Engineer Portfolio

End-to-end data engineering projects showcasing scalable data pipelines, cloud-native architectures, and analytics-ready datasets using AWS, Spark, and Python.

---

## ðŸ‘‹ About Me

I am a Data Engineer with 4+ years of experience designing and building scalable, cloud-native data platforms. I specialize in Python, SQL, and PySpark, with hands-on experience across batch and CDC pipelines, data modeling, and analytics enablement.

My background includes building production data systems on AWS for healthcare and insurance datasets, where data quality, security, and compliance are critical. I focus on creating reliable, performant pipelines that transform raw data into trusted, analytics- and ML-ready datasets.

---

## ðŸ›  Skills

**Languages:** Python, SQL, PySpark  
**Big Data:** Apache Spark, AWS Glue, Airflow, DBT  
**Cloud (AWS):** S3, Glue, Redshift, EMR, Lambda, EventBridge, IAM  
**Databases & Warehouses:** Snowflake, Amazon Redshift, PostgreSQL  
**Data Modeling:** OLTP, OLAP, Star Schema  
**DevOps:** Docker, Terraform, CI/CD  
**BI & Analytics:** Power BI, Tableau  
**Governance:** HIPAA, GDPR (data handling exposure)

---

## ðŸ“Œ Portfolio Projects

### ðŸ©º Healthcare Claims Data Platform (AWS)
- Built an end-to-end data pipeline to ingest, process, and curate healthcare insurance claims data.
- Implemented batch and CDC ingestion with late-arriving data handling and data quality checks.
- Delivered analytics-ready and ML-ready datasets for downstream BI and fraud detection use cases.

**Tech Stack:** AWS S3, AWS Glue, PySpark, Snowflake, Airflow

---

### ðŸ”„ Change Data Capture (CDC) Pipeline
- Designed a CDC pipeline to capture incremental changes from a transactional database into a data lake.
- Ensured idempotent loads, deduplication, and historical data preservation.
- Optimized Spark jobs for large-scale incremental processing.

**Tech Stack:** PostgreSQL, AWS DMS, S3, PySpark, Snowflake

---

### âš¡ Event-Driven Data Processing
- Built an event-driven pipeline triggered by object uploads to Amazon S3.
- Automated near real-time transformations using AWS Lambda and Glue.
- Reduced processing latency and improved system responsiveness.

**Tech Stack:** S3, EventBridge, Lambda, Glue

---

## ðŸ“‚ Repository Structure

```text
data-engineer-portfolio/
â”‚
â”œâ”€â”€ 01_sql/
â”œâ”€â”€ 02_python/
â”œâ”€â”€ 03_spark_pyspark/
â”œâ”€â”€ 04_cloud_aws/
â”œâ”€â”€ 05_end_to_end_projects/
â”œâ”€â”€ 06_devops/
â””â”€â”€ datasets/
